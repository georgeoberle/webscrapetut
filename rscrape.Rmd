---
title: "rscrape"
author: "George Oberle"
date: "January 14, 2015"
output: html_document
---

This is a tutorial meant for historians to use in order to use the rvest package to scrape the web.

# Harvesting the Web using r


##What is rvest?

There are many useful sites that contain pieces of information that inform or are useful to your work. The ability to extract meaningful data is a useful tool to include in your toolbox. R has a package called rvest that allows you to simply scrape a website in order to use the content in a way that is more meaningful to your own work. The tool is free and open and works well within the RStudio environment.

##Access and Install

Access to the package is from Hadley Wickham’s GitHub (https://github.com/hadley/rvest) or from CRAN.



TODO: Add command to install. Note, [rvest](http://cran.rstudio.org/web/packages/rvest/) is on CRAN now.

##Basics

Wickham created the tool in order to allow for typical web scraping tasks that many python users are familiar with. There are several good lessons available at The Programming Historian for those that are more familiar with python. The advantage of rvest is that it requires less time to understand the nuances of the programming language so the scholar can spend more time considering the crux of their scholarship. It is designed to work with magrittr which helps simplify your code. Rvest allows users to connect to a site within R and then grab parts (or all) of a site. Users can navigate to websites using rstudio while connected in rvest.

There are excellent tutorials on other tools available at *The Programming Historian* The url is http://programminghistorian.org/

The rvest documentation says  that users can do the following:

* Create an html document from a url, a file on disk or a string containing html with html().
* Select parts of a document using css selectors: html_nodes(doc, "table td")
* Extract components with html_tag() (the name of the tag), html_text() (all text inside the tag), html_attr() (contents of a single attribute) and html_attrs() (all attributes).
* Parse tables into data frames with html_table().
* Extract, modify and submit forms with html_form(), set_values() and submit_form()
* Navigate around a website as if you’re in a browser with html_session(), jump_to(), follow_link(), back(), forward(), submit_form() and so on.

###Some Packages that may be useful and why

**rvest:** rvest makes it easy to harvest (a.k.a. scrape) data from web pages by providing some helpful wrappers around XML and httr. This package depends on R 3.0.1 or greater. There is a built in demo that shows users how the codes work.

```{r}
library(rvest)
```
Also helpful is magrittr: magrittr is a forward pipe function that allows the rvest user to construct programming statements. The following symbol %>% is used to combine functions.

```{r}
library(magrittr)
```

Optional Packages

stringr: is a set of simple wrappers that make R’s string functions more consistent, simpler and easier to use.  It does this by ensuring that: function and argument names (and positions) are consistent, all functions deal with NA’s and zero length character appropriately, and the output data structures from each function matches the input data structures of other functions.

```{r}
library(stringr)
```

-----

##Examples for Historians

I plan to develop several examples of common types of sources that historians encounter. Here are key library packages that may be helpful to load.

Tools needed
```{r}
library (rvest)
library (magrittr)
library(stringr)
```

###Example 1:

*Virginia County Records* (Library of Virginia) http://www.lva.virginia.gov/public/local/

The Library of Virginia provides several crucial resources for historians. Several of these resources include guides to materials available from the Library of Virginia. These guides are available on the Library of Virginia's website. Some of these guides are created in tables on html pages. A good example is the site titled *A Guide to Virginia County and City Records on Microfilm*. These records are arranged by the locality. So this means that they are organized by county unless it has independent city status. The guides typically include columns that indicate a reel number and if there is a copy available for loan via interlibrary loan. Additionally there is a column that describes the content of the reels using the descriptor "Local Records Categories and Contents."

Using rstudio the first step is to esablish a connection with a site.

```{r}
lva  <- html_session ("http://www.lva.virginia.gov/whatwehave/local/county_formation/index.htm")
```

This establishes a session and can be verified by using the class command.

```{r}
class (lva)
```


Think of this as a live internet connection with the site specified. This session can also be established by simply typing in the specific url and then chained with specific commands as seen in the next section.

####Scraping a page.

Use the html_table command to parse the html into a data frame.

```{r}
vacountstaf  <- "http://www.lva.virginia.gov/public/local/results_all.asp?CountyID=VA265"
vacountstaf %>%
  html() %>%
  html_node ("table") %>% 
  html_table () %>%
  as.data.frame() %>%
  write.csv("stafford.csv")
  
```

You will see that the data is not perfect but it has been Now that this data is saved into a csv file it is can easily cleaned and made into tidy data using Hadley Wickham's packaged dplyr and tidr.

###Example 2: Carnegie Libraries
Much of my work is focused the establishment, growth, and diversification of knowledge institutions. These are typically associated with cultural institutions like libraries, museums, archives, colleges and universities, and learned societies. The data that documents these institutions is typically dispursed in many different locations and does not cover a significant span of time. There are many sites dedicated to these types of institutions like the venerable *Scholarly Societies Site* (http://www.scholarly-societies.org/) made available from the University of Waterloo Library. Another useful place to find lists and tables has been wikipedia. Like all sites it is crucial to examine where the data is derived from but after checking the source this may be a good option for historians. For example, the wikipedia entry on "Carnegie Libraries in the United States" 


```{r}
carnlib  <- "https://en.wikipedia.org/wiki/List_of_Carnegie_libraries_in_the_United_States"
carnlib %>%
  html() %>%
  html_node (xpath='//*[@id="mw-content-text"]/table[1]') %>% 
  html_table() %>%
  as.data.frame() %>%
  write.csv("carnegielibrary.csv")
 
```



####House of Representatives Party Divisions

The site (http://history.house.gov/Institution/Party-Divisions/Party-Divisions/) is made available from the Historians of the Office of the House of Representatives titled *Party Divisions of the House of Representatives: 1789–Present*

It documents the party membership of members of the U.S. House of Representatives and is derived from several sources including *The Biographical Directory of the U.S. Congress*, the House Clerk’s Election Statistics, Congressional Quarterly’s *Guide to U.S. Elections*, Michael Dubin’s *United States Congressional Elections*, and Kenneth Martis’s *Historical Atlas of Political Parties in the United States Congress*.

####Establish a session and create a table from the site.
```{r}
#house_party<-html_session("http://history.house.gov/Institution/Party-Divisions/Party-Divisions/")

```

###Example 3: Archival Finding Aid structured using Encoded Archival Description (EAD)

The American Philosophical Society Archives finding aid: The Archives of the APS contains records of the first learned society founded in the United States. The finding aid is available at (http://www.amphilsoc.org/mole/view?docId=ead/APS.Archives-ead.xml#d26860101e8658654068736). The APS records is organized into thirteen record groups dating back to 1743. The Society's archives extensively documents not only the organization's historical development but also its role in American history and the history of science and technology.
As with the other examples begin by establishing a session.
```{r}
 #apsead<- html_session ("http://www.amphilsoc.org/mole/view?docId=ead/APS.Archives-ead.xml#d26860101e8658654068736")
```

This establishes the session on the American Philosophical Society's Library web site. Next view the css coding. Assuming we only want to bring in the section of the site with the detailed inventory select the .compMargin01 element.


apsead %>%
    html() %>%
    html_nodes(".compMargin01") %>% 
    html_text () %>%
  as.data.frame() %>%
  write.csv("aps.csv")

```{r}
#apsead1<- html_session ("http://www.amphilsoc.org/mole/view?docId=ead/APS.Archives-ead.xml#d26860101e8658654068736")
#apseadelement1<- apsead %>%
#html_nodes(".abstract p") %>%
#html_table()
```

```{r}
#apsead<- html_session ("http://www.amphilsoc.org/mole/view?docId=ead/APS.Archives.IIa-ead.xml")
#apseadelement2<- apsead %>%
#html_nodes(".div.content tr") %>%
#html_table()
```





